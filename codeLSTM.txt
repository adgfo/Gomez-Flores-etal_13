### Thank you for looking at our code. Please cite our work when you use this code, thanks
### 1. Database (you can find it in our repository https://github.com/adgfo/Gomez-Flores-etal_13)
import pandas as pd # pd version 2.1.4
df=pd.read_csv('Database0_SS.csv')
### 2. Data processing
import numpy as np # np version 1.26.2
from sklearn.preprocessing import MinMaxScaler, QuantileTransformer # scikit-learn version 1.3.0
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
class NegativeTransformer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):        
        return self
    def transform(self, X):        
        return X * -1
    def inverse_transform(self, X):
        return X * -1
samples=round(len(df['H_nm']))
scaler1=QuantileTransformer(n_quantiles=int(samples*0.2), output_distribution='uniform', subsample=int(samples), random_state=1234) #  A131, a1, T: 0, 1
scaler2=MinMaxScaler(feature_range=(-1, 1)) # zeta: -1, 1
scaler3=QuantileTransformer(n_quantiles=int(samples*0.2), output_distribution='uniform', subsample=int(samples), random_state=1234) # c, z, H: 0, 1
scaler4=Pipeline([('quant', QuantileTransformer(n_quantiles=int(samples*0.2), output_distribution='uniform', subsample=int(samples), random_state=1234)),
                  ('neg', NegativeTransformer())]) # vdw: -1, 0
scaler5=Pipeline([('quant', QuantileTransformer(n_quantiles=int(samples*0.2), output_distribution='normal', subsample=int(samples), random_state=1234)),
                  ('minmax', MinMaxScaler(feature_range=(-1, 1)))]) # els: -1, 1
x1=scaler1.fit_transform(df[['A131_J', 'AR_m', 'T_K']])
x2=scaler2.fit_transform(df[['Z1_V']])
x3=scaler3.fit_transform(df[['IS_M', 'z', 'H_nm']])
y1=scaler4.fit_transform(df[['VDW_SS_kT']])
y2=scaler5.fit_transform(df[['ELS_SS_kT']])
x=np.hstack((x1, x2, x3))
y=np.hstack((y1, y2))
len1=df['H_nm'].value_counts().get(1.000, 0)
len2=int(len(df['H_nm'])/len1)
x_shape=x.reshape(x.shape[0], 1, x.shape[1])
take=8900
x_train, x_test=x_shape[0:take*len2, :], x_shape[take*len2:, :]
y_train, y_test=y[0:take*len2, :], y[take*len2:, :]
### 3. LSTM modeling
import tensorflow as tf # tensorflow version 2.10.1
from tensorflow import keras # keras version 2.10.0
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import RMSprop
model=Sequential()
model.add(LSTM(units=256, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]), kernel_initializer='glorot_uniform', recurrent_initializer='random_normal'))
model.add(Dropout(0.1))
model.add(LSTM(units=256, return_sequences=True, kernel_initializer='glorot_uniform', recurrent_initializer='random_normal'))
model.add(Dropout(0.1))
model.add(LSTM(units=128, return_sequences=True, kernel_initializer='glorot_uniform', recurrent_initializer='random_normal'))
model.add(Dropout(0.1))
model.add(LSTM(units=64, return_sequences=False, kernel_initializer='glorot_uniform', recurrent_initializer='random_normal'))
model.add(Dropout(0.1))
model.add(Dense(units=y_train.shape[1], activation='tanh', kernel_initializer='glorot_uniform', bias_initializer='random_normal'))
opt=RMSprop(learning_rate=0.001, rho=0.90, momentum=0.90, epsilon=1E-7, centered=False)
model.compile(optimizer=opt, loss='mean_squared_error')
loss_t_histories=[]
loss_v_histories=[]
for i in range(12):
    # 3.1. Train
    history=model.fit(x_train, y_train, epochs=210, batch_size=16384, validation_data=(x_test, y_test), shuffle=True, verbose=0)
    # 3.2. Test
    loss_v=model.evaluate(x_test, y_test, verbose=0)    
    # 3.3. Loss histories
    loss_t_histories.append(history.history['loss'])
    loss_v_histories.append(history.history['val_loss'])
